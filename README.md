# Repository To RAG ğŸš€

A powerful tool for creating and managing vector embeddings of code repositories. This project helps developers create searchable vector databases of their codebase using OpenAI embeddings and Pinecone vector store.

## Features âœ¨

- ğŸ“š Efficient repository cloning and caching
- ğŸ” Smart text and code chunking with context preservation
- ğŸš„ High-performance vector embeddings using OpenAI
- âš¡ Optimized Pinecone uploads using gRPC
- ğŸ’¾ Intelligent caching system for processed files
- ğŸ”„ Automatic file type detection and encoding
- ğŸ“¦ Support for multiple programming languages

## Installation ğŸ› ï¸

1. Clone this repository:
```bash
git clone https://github.com/elanrad1/Git-to-RAG.git
cd repository-code-embedder
```

2. Create a virtual environment and activate it:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file with your API keys:
```bash
OPENAI_API_KEY=your_openai_api_key
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_REGION=your_pinecone_region
```

## Usage ğŸ¯

1. Configure your repository settings in `main.py`:
```python
repo_url = "https://github.com/username/repository.git"
target_folder = "path/to/folder"  # Optional
model = "gpt-3.5-turbo"  # Model for tokenization
index_name = "your-index-name"
```

2. Run the embedder:
```bash
python main.py
```

The tool will:
- Clone the repository
- Process and chunk the code files
- Create embeddings using OpenAI
- Store vectors in Pinecone
- Cache results for future runs

## Architecture ğŸ—ï¸

- `src/repo_cloner.py`: Handles repository cloning and caching
- `src/chunker.py`: Processes files into optimized chunks
- `src/pinecone_uploader.py`: Manages vector embeddings and uploads
- `src/config.py`: Centralizes configuration management
- `src/utils.py`: Provides utility functions and helpers

## Performance Optimizations ğŸ”¥

- Parallel processing for batch uploads using ThreadPoolExecutor
- gRPC implementation for faster Pinecone operations
- Efficient caching system for incremental updates
- Smart chunking with language-specific separators
- Optimized batch sizes for vector uploads (200 vectors per batch)
- Async requests for better throughput
- Progress bars for monitoring long operations

## Caching System ğŸ“¦

The tool implements a sophisticated caching system that:
- Stores processed chunks in JSON format
- Uses MD5 hashing for file change detection
- Maintains separate metadata for repos and chunks
- Implements safe directory operations with retries
- Only reprocesses modified files
- Preserves file metadata across runs

## File Processing ğŸ“„

Supports multiple file types with specialized handling:
```python
default_extensions = {
    '.py', '.js', '.java', '.cpp', '.c', '.h', 
    '.cs', '.php', '.rb', '.go', '.txt', '.md',
    '.rst', '.json', '.yml', '.yaml', '.toml',
    '.ini', '.cfg', '.conf'
}
```

Features:
- Automatic encoding detection
- Language-specific chunking strategies
- Code-aware text splitting
- Token counting using model tokenizer
- Metadata preservation

## Vector Upload System âš¡

Implements high-performance vector uploads:
- Uses Pinecone gRPC client for speed
- Parallel batch processing
- Async requests with error handling
- Progress tracking with tqdm
- Automatic index creation if needed
- Configurable batch sizes and workers

## Contributing ğŸ¤

Contributions are welcome! Please feel free to submit a Pull Request.

## License ğŸ“

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments ğŸ™

- OpenAI for embeddings
- Pinecone for vector storage
- LangChain for utilities